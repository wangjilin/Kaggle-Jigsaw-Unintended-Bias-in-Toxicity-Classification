{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is a fork of [this](https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version) kernel made to work on Fast.AI and <br>\n",
    "Uses Weighted BCE Loss as described in [this](https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution) kernel. <br>\n",
    "Other than that nothing else has been changed. All improvemnts mentioned [here](https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version) could still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "# def build_matrix(word_index, path):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "#     unknown_words = []\n",
    "    \n",
    "#     for word, i in word_index.items():\n",
    "#         try:\n",
    "#             embedding_matrix[i] = embedding_index[word]\n",
    "#         except KeyError:\n",
    "#             unknown_words.append(word)\n",
    "#     return embedding_matrix, unknown_words\n",
    "\n",
    "def build_matrix(word_index, embedding_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_model(learn,test,output_dim,lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    n = len(learn.data.train_dl)\n",
    "    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n",
    "    sched = GeneralScheduler(learn, phases)\n",
    "    learn.callbacks.append(sched)\n",
    "    for epoch in range(n_epochs):\n",
    "        learn.fit(1)\n",
    "        test_preds = np.zeros((len(test), output_dim))    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            X = x_batch[0].cuda()\n",
    "            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(data):\n",
    "#     '''\n",
    "#     Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "#     '''\n",
    "#     punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'\n",
    "#     def clean_special_chars(text, punct):\n",
    "#         for p in punct:\n",
    "#             text = text.replace(p, ' ')\n",
    "#         return text\n",
    "\n",
    "#     data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train ,test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean website address\n",
    "def clean_web(text):\n",
    "    temp1 = text.split()\n",
    "    for i in range(len(temp1)):\n",
    "        if '://mobile' in  temp1[i][:9]:\n",
    "#             print(temp1[i])\n",
    "            temp1[i] = ''\n",
    "        elif 'ttps://' in temp1[i][:7]:\n",
    "#             print(temp1[i])\n",
    "            temp1[i] = ''\n",
    "        elif 'Http:' in temp1[i] or 'Https:' in temp1[i]:\n",
    "            pos = temp1[i].find('Http')\n",
    "            temp1[i] = temp1[i][:pos]\n",
    "        elif 'http:' in temp1[i] or 'https:' in temp1[i]:\n",
    "            pos = temp1[i].find('http')\n",
    "            temp1[i] = temp1[i][:pos]\n",
    "        elif 'www.' == temp1[i][:4]:  # just in case like 'Awww...I', so I'm not using 'www.' in temp1[i]\n",
    "            temp1[i] = ''\n",
    "    return ' '.join(temp1)\n",
    "\n",
    "temp = df['comment_text'].apply(lambda x: clean_web(x))\n",
    "no_web_comment_text = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_vocab(texts):\n",
    "#     sentences = texts.apply(lambda x: x.split()).values\n",
    "#     vocab = {}\n",
    "#     for sentence in tqdm(sentences):\n",
    "#         for word in sentence:\n",
    "#             try:\n",
    "#                 vocab[word] += 1\n",
    "#             except KeyError:\n",
    "#                 vocab[word] = 1\n",
    "#     return vocab\n",
    "# first add lower based on no_web_comment_text\n",
    "temp = temp.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct Contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n",
    "                       \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "temp = temp.apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all Chinese, Japanese and Korean Characters\n",
    "def remove_Asia_letters(check_str):\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff' or u'\\uac00' <= ch <= u'\\ud7ff' or u'\\u3040' <= ch <= u'\\u30ff':\n",
    "            check_str = check_str.replace(ch,' ')\n",
    "    return check_str\n",
    "\n",
    "temp = temp.apply(lambda x: remove_Asia_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def demojize_text(check_str):\n",
    "    for ch in check_str:\n",
    "        if ch in emoji.UNICODE_EMOJI:\n",
    "            return emoji.demojize(check_str)\n",
    "    return check_str\n",
    "\n",
    "temp = temp.apply(lambda x: demojize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'\n",
    "punct_mapping = {\"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \", \"√ó\": \"x\", \n",
    "                 \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\", \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"', '‚Äù': '\"', '‚Äú': '\"', \n",
    "                 \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', 'Œ±': 'alpha', '‚Ä¢': '.', '√†': 'a', \n",
    "                 '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi', '·¥Ä…¥·¥Ö':'and','·¥Ä':'a','na√Øve':'naive',\n",
    "                '‚Äï':'-',' ú·¥è·¥ç·¥á':'home','·¥ú·¥ò':'up',' ô è':'by','y·¥è·¥ú':'you','·¥Ä·¥õ':'at','·¥Ñ·¥è·¥ç·¥ò·¥ú·¥õ·¥á Ä':'computer','·¥õ ú…™s':'this',\n",
    "                 '·¥ç·¥è…¥·¥õ ú':'month','·¥°·¥è Ä·¥ã…™…¥…¢':'working','chr√©tien':'chretien','·¥ä·¥è ô':'job','·¥è“ì':'of',' ú·¥è·¥ú Ä ü è':'hourly',\n",
    "                 '·¥°·¥á·¥á·¥ã':'week',' ü…™…¥·¥ã':'link','·¥õ·¥è':'to',' ú·¥Ä·¥†·¥á':'have','·¥Ñ·¥Ä…¥':'can','·¥á…¥·¥Ö':'end','“ì…™ Äs·¥õ':'first',\n",
    "                 ' è·¥è·¥ú Ä':'your','s…™…¢…¥…™…¥…¢':'signing',' ô·¥è·¥õ·¥õ·¥è·¥ç':'bottom','“ì·¥è ü ü·¥è·¥°…™…¥…¢':'following','m·¥Ä·¥ã·¥á':'make',\n",
    "                 '·¥Ñ·¥è…¥…¥·¥á·¥Ñ·¥õ…™·¥è…¥':'connection','…™…¥·¥õ·¥á Ä…¥·¥á·¥õ':'internet',' Ä·¥á ü…™·¥Ä ô ü·¥á':'reliable','…¥·¥á·¥á·¥Ö':'need','·¥è…¥ ü è':'only',\n",
    "                 '…™…¥·¥Ñ·¥è·¥ç·¥á':'income','·¥áx·¥õ Ä·¥Ä':'extra','·¥Ä…¥':'an','…¥·¥á·¥á·¥Ö…™…¥…¢':'needing','·¥Ä…¥ è·¥è…¥·¥á':'anyone','·¥è Ä':'or',\n",
    "                 '·¥ç·¥è·¥çs':'moms','s·¥õ·¥Ä è':'stay','s·¥õ·¥ú·¥Ö·¥á…¥·¥õs':'students','g Ä·¥á·¥Ä·¥õ':'great','“ì Ä·¥è·¥ç':'from','s·¥õ·¥Ä Ä·¥õ':'start',\n",
    "                 'qu√©bec':'quebec','ùíÇùíèùíÖ':'and','brexit':'british exit','¬ª':'>>','¬´':'<<','¬∑':'.','co‚ÇÇ':'co2',\n",
    "                'clich√©':'cliche','¬Ω':'1/2','‚Ä∫':'>','‚ô°':'love','‚ú¨':'star','·¥õ ú·¥á':'the','a·¥õ':'at',' úa·¥†·¥á':'have',\n",
    "                 '·¥Ña…¥':'can',' ô·¥è·¥õto·¥ç':'bottom','ma·¥ã·¥á':'make',' Ä·¥á ü…™a ô ü·¥á':'reliable','·¥áx·¥õ Äa':'extra','a…¥':'an',\n",
    "                 'need…™…¥…¢':'needing','a…¥ è·¥è…¥·¥á':'anyone','s·¥õa è':'stay','g Ä·¥áa·¥õ':'great','s·¥õa Ä·¥õ':'start','Œ¨':'a', 'ùì¥':'k',\n",
    "                 'ùò¢':'a','√£':'a', 'Ô¨Ç':'fl', 'ƒ•':'h','ùó≤':'e', 'ùíê':'o', 'üá≥':'n', 'ùíó':'v','‚íä':'3.','√Ø':'i', 'ùôú':'g', \n",
    "                 'Œª':'lambda', 'ùëπ':'r','ÔΩé':'n', '¬°':'i', 'ùñó':'r', 'ùëæ':'w', 'ùíñ':'u', 'ùòÜ':'y', '!':'!', \n",
    "                 'üáº':'w', '–π':'n', 'ùòß':'f', '·¥ò':'p','ùìâ':'t', 'ùüê':'2','Ô¨É':'ffi','ƒâ':'c', '·ë≠':'rho', 'ùñå':'g', \n",
    "                 '–ø':'pi', '‡±¶':'o', 'ùëÆ':'g','Œæ':'xi','·º∞':'i', '·ëØ':'d','üáß':'b', 'ùíë':'p', 'ùìä':'u', '–Ω':'h', 'ùí∏':'c',\n",
    "                 'ùòº':'a','ùò≤':'q', 'ÔΩï':'u', 'ƒï':'e', 'ùôõ':'f','ŒΩ':'v', '◊û':'n', 'ƒ≠':'i', '«ê':'i','√•':'a', 'ùìΩ':'t',\n",
    "                 '·º¥':'i', 'œç':'u', 'ƒá':'c', '√§':'a', '≈ô':'r', 'ƒ°':'g', 'ùìµ':'l','üáª':'v', 'ùíï':'t','ùò¨':'k','ùó∏':'k', 'ÔΩÅ':'a',\n",
    "                 '‚≤è':'h', '·¥Ö':'d', '“Ø':'gamma', 'ùñâ':'d', '–µ':'e','üá´':'f', 'ùìª':'r', 'ùô™':'u', '·¥Ñ':'c','œá':'x','ùëª':'t', \n",
    "                 'ùë•':'x', 'ùíá':'f', 'ƒÅ':'a', 'ùòµ':'t','ÔΩÑ':'d', '…¥':'n','ùëØ':'h', 'ÔΩÉ':'c','—à':'w','ùíÅ':'z', 'ùê´':'r', \n",
    "                 'ùì≤':'i', 'ùñà':'c', 'ÔΩê':'p', '·¥¶':'r','ùñï':'p', '√™':'e','ùñÇ':'b', 'ùêØ':'v','ùìÄ':'k',  'ùìÆ':'e', 'ùóû':'k', \n",
    "                 'ùíÉ':'b', '·¥ç':'m', 'œÅ':'rho', 'ùí∂':'a', '‚Ñ¥':'o', '·øñ':'i', 'ùóµ':'h', 'ùôØ':'z', '√¥':'o', 'œÑ':'tau', 'ùê©':'p', 'Í≠ª':'j',\n",
    "                '\\xad':' ','‚îà':'....',' ª':\"'\",'√º':'u','√©':'e','·¥á':'e','√®':'e','“ì':'f','ùíä':'i',' è':'y','ùíÇ':'a','·¥ã':'k','ùíè':'n',\n",
    "                 '–∞':'a','≈õ':'s', '√≠':'i','–æ':'o','\\x7f':' ','ùíî':'s','ùôö':'e','√∂':'o','–≤':'b','s':'s','‚ñÄ':' ','‚ñÑ':' ','‚û§':' ',\n",
    "                 '‚ïê':'=','‚òª':' smile ','‚ù•':' love ','‚òÖ':' star ','–º':'m','ùô£':'n','ùíÖ':'d','ùôß':'r','ùíÑ':'c','≈ç':'o','ùô®':'s','·¥õ':'t',\n",
    "                 'ƒì':'e','·¥ú':'u','ùíé':'m','ùôû':'i','œÖ':'u','…™':'i','–∫':'k','‚ñ∞':' ','‚ñî':' ','‚ñ¨':' ','Ã∂':'-','‚ï≤':\"\\\\\",'‚ï±':'/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ') # we can try just replace puncts into space\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '‚Ä¶': ' ... ', '\\ufeff': '', '‡§ï‡§∞‡§®‡§æ': '', '‡§π‡•à': '','‚ú∞':' ','¬ß':' ','‚óã':'','‚ùß':'','Œπ':''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "temp = temp.apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = temp[:train.shape[0]]\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = temp[train.shape[0]:]\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "pickle_out = open('x_train.pickle','wb')\n",
    "pickle.dump(x_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_aux_train.pickle','wb')\n",
    "pickle.dump(y_aux_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('x_test.pickle','wb')\n",
    "pickle.dump(x_test,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('weights.pickle','wb')\n",
    "pickle.dump(weights,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('loss_weight.pickle','wb')\n",
    "pickle.dump(loss_weight,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_train.pickle','wb')\n",
    "pickle.dump(y_train,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del test\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = preprocess(train['comment_text'])\n",
    "# y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "# x_test = preprocess(test['comment_text'])\n",
    "\n",
    "# identity_columns = [\n",
    "#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# # Overall\n",
    "# weights = np.ones((len(x_train),)) / 4\n",
    "# # Subgroup\n",
    "# weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# # Background Positive, Subgroup Negative\n",
    "# weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# # Background Negative, Subgroup Positive\n",
    "# weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "# y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity_columns = [\n",
    "#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# # Overall\n",
    "# weights = np.ones((len(x_train),)) / 4\n",
    "# # Subgroup\n",
    "# weights += (train_identity_columns.fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# # Background Positive, Subgroup Negative\n",
    "# weights += (( (train_target.values>=0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# # Background Negative, Subgroup Positive\n",
    "# weights += (( (train_target.values<0.5).astype(bool).astype(np.int) +\n",
    "#    (train_identity_columns.fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "# y_train = np.vstack([(train_target.values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282482"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_no_web = build_vocab(no_web_comment_text)\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 31155 words to embedding\n"
     ]
    }
   ],
   "source": [
    "C_embedding = load_embeddings(CRAWL_EMBEDDING_PATH)\n",
    "add_lower(C_embedding, vocab_no_web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  110238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, C_embedding)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "del C_embedding\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 25381 words to embedding\n",
      "n unknown words (glove):  111000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_embedding = load_embeddings(GLOVE_EMBEDDING_PATH)\n",
    "add_lower(G_embedding, vocab_no_web)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, G_embedding)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "del G_embedding\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vocab_no_web\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_out = open('embedding_matrix.pickle','wb')\n",
    "pickle.dump(embedding_matrix,pickle_out)\n",
    "pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (os.listdir('./'))\n",
    "# pickle_in = open('embedding_matrix.pickle','rb')\n",
    "# test_matrix = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embedding_matrix.shape)\n",
    "# print(test_matrix.shape)\n",
    "# # \n",
    "# np.array_equal(embedding_matrix,test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(x_train)\n",
    "# pickle_out = open('x_train.pickle','wb')\n",
    "# pickle.dump(x_train,pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (os.listdir('./'))\n",
    "# pickle_in = open('x_train.pickle','rb')\n",
    "# test_matrix2 = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array_equal(x_train,test_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_torch = torch.tensor(x_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "\n",
    "# train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "# valid_dataset = data.TensorDataset(x_train_torch[:batch_size], y_train_torch[:batch_size])\n",
    "# test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# databunch = DataBunch(train_dl=train_loader,valid_dl=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(data, targets):\n",
    "#     ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "#     bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "#     bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "#     return (bce_loss_1 * loss_weight) + bce_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_test_preds = []\n",
    "\n",
    "# for model_idx in range(NUM_MODELS):\n",
    "#     print('Model ', model_idx)\n",
    "#     seed_everything(1234 + model_idx)\n",
    "#     model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "#     learn = Learner(databunch,model,loss_func=custom_loss)\n",
    "#     test_preds = train_model(learn,test_dataset,output_dim=7)    \n",
    "#     all_test_preds.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame.from_dict({\n",
    "#     'id': test['id'],\n",
    "#     'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "# })\n",
    "\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public LB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
