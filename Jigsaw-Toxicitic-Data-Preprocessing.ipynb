{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is a fork of [this](https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version) kernel made to work on Fast.AI and <br>\n",
    "Uses Weighted BCE Loss as described in [this](https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution) kernel. <br>\n",
    "Other than that nothing else has been changed. All improvemnts mentioned [here](https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version) could still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "# def build_matrix(word_index, path):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "#     unknown_words = []\n",
    "    \n",
    "#     for word, i in word_index.items():\n",
    "#         try:\n",
    "#             embedding_matrix[i] = embedding_index[word]\n",
    "#         except KeyError:\n",
    "#             unknown_words.append(word)\n",
    "#     return embedding_matrix, unknown_words\n",
    "\n",
    "def build_matrix(word_index, embedding_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_model(learn,test,output_dim,lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    n = len(learn.data.train_dl)\n",
    "    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n",
    "    sched = GeneralScheduler(learn, phases)\n",
    "    learn.callbacks.append(sched)\n",
    "    for epoch in range(n_epochs):\n",
    "        learn.fit(1)\n",
    "        test_preds = np.zeros((len(test), output_dim))    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            X = x_batch[0].cuda()\n",
    "            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(data):\n",
    "#     '''\n",
    "#     Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "#     '''\n",
    "#     punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "#     def clean_special_chars(text, punct):\n",
    "#         for p in punct:\n",
    "#             text = text.replace(p, ' ')\n",
    "#         return text\n",
    "\n",
    "#     data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train ,test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean website address\n",
    "def clean_web(text):\n",
    "    temp1 = text.split()\n",
    "    for i in range(len(temp1)):\n",
    "        if '://mobile' in  temp1[i][:9]:\n",
    "#             print(temp1[i])\n",
    "            temp1[i] = ''\n",
    "        elif 'ttps://' in temp1[i][:7]:\n",
    "#             print(temp1[i])\n",
    "            temp1[i] = ''\n",
    "        elif 'Http:' in temp1[i] or 'Https:' in temp1[i]:\n",
    "            pos = temp1[i].find('Http')\n",
    "            temp1[i] = temp1[i][:pos]\n",
    "        elif 'http:' in temp1[i] or 'https:' in temp1[i]:\n",
    "            pos = temp1[i].find('http')\n",
    "            temp1[i] = temp1[i][:pos]\n",
    "        elif 'www.' == temp1[i][:4]:  # just in case like 'Awww...I', so I'm not using 'www.' in temp1[i]\n",
    "            temp1[i] = ''\n",
    "    return ' '.join(temp1)\n",
    "\n",
    "temp = df['comment_text'].apply(lambda x: clean_web(x))\n",
    "no_web_comment_text = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_vocab(texts):\n",
    "#     sentences = texts.apply(lambda x: x.split()).values\n",
    "#     vocab = {}\n",
    "#     for sentence in tqdm(sentences):\n",
    "#         for word in sentence:\n",
    "#             try:\n",
    "#                 vocab[word] += 1\n",
    "#             except KeyError:\n",
    "#                 vocab[word] = 1\n",
    "#     return vocab\n",
    "# first add lower based on no_web_comment_text\n",
    "temp = temp.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct Contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n",
    "                       \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "temp = temp.apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all Chinese, Japanese and Korean Characters\n",
    "def remove_Asia_letters(check_str):\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff' or u'\\uac00' <= ch <= u'\\ud7ff' or u'\\u3040' <= ch <= u'\\u30ff':\n",
    "            check_str = check_str.replace(ch,' ')\n",
    "    return check_str\n",
    "\n",
    "temp = temp.apply(lambda x: remove_Asia_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def demojize_text(check_str):\n",
    "    for ch in check_str:\n",
    "        if ch in emoji.UNICODE_EMOJI:\n",
    "            return emoji.demojize(check_str)\n",
    "    return check_str\n",
    "\n",
    "temp = temp.apply(lambda x: demojize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \n",
    "                 \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', \n",
    "                 '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', 'ᴀɴᴅ':'and','ᴀ':'a','naïve':'naive',\n",
    "                '―':'-','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','yᴏᴜ':'you','ᴀᴛ':'at','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer','ᴛʜɪs':'this',\n",
    "                 'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','chrétien':'chretien','ᴊᴏʙ':'job','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly',\n",
    "                 'ᴡᴇᴇᴋ':'week','ʟɪɴᴋ':'link','ᴛᴏ':'to','ʜᴀᴠᴇ':'have','ᴄᴀɴ':'can','ᴇɴᴅ':'end','ғɪʀsᴛ':'first',\n",
    "                 'ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','mᴀᴋᴇ':'make',\n",
    "                 'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','ʀᴇʟɪᴀʙʟᴇ':'reliable','ɴᴇᴇᴅ':'need','ᴏɴʟʏ':'only',\n",
    "                 'ɪɴᴄᴏᴍᴇ':'income','ᴇxᴛʀᴀ':'extra','ᴀɴ':'an','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴʏᴏɴᴇ':'anyone','ᴏʀ':'or',\n",
    "                 'ᴍᴏᴍs':'moms','sᴛᴀʏ':'stay','sᴛᴜᴅᴇɴᴛs':'students','gʀᴇᴀᴛ':'great','ғʀᴏᴍ':'from','sᴛᴀʀᴛ':'start',\n",
    "                 'québec':'quebec','𝒂𝒏𝒅':'and','brexit':'british exit','»':'>>','«':'<<','·':'.','co₂':'co2',\n",
    "                'cliché':'cliche','½':'1/2','›':'>','♡':'love','✬':'star','ᴛʜᴇ':'the','aᴛ':'at','ʜaᴠᴇ':'have',\n",
    "                 'ᴄaɴ':'can','ʙᴏᴛtoᴍ':'bottom','maᴋᴇ':'make','ʀᴇʟɪaʙʟᴇ':'reliable','ᴇxᴛʀa':'extra','aɴ':'an',\n",
    "                 'needɪɴɢ':'needing','aɴʏᴏɴᴇ':'anyone','sᴛaʏ':'stay','gʀᴇaᴛ':'great','sᴛaʀᴛ':'start','ά':'a', '𝓴':'k',\n",
    "                 '𝘢':'a','ã':'a', 'ﬂ':'fl', 'ĥ':'h','𝗲':'e', '𝒐':'o', '🇳':'n', '𝒗':'v','⒊':'3.','ï':'i', '𝙜':'g', \n",
    "                 'λ':'lambda', '𝑹':'r','ｎ':'n', '¡':'i', '𝖗':'r', '𝑾':'w', '𝒖':'u', '𝘆':'y', '!':'!', \n",
    "                 '🇼':'w', 'й':'n', '𝘧':'f', 'ᴘ':'p','𝓉':'t', '𝟐':'2','ﬃ':'ffi','ĉ':'c', 'ᑭ':'rho', '𝖌':'g', \n",
    "                 'п':'pi', '౦':'o', '𝑮':'g','ξ':'xi','ἰ':'i', 'ᑯ':'d','🇧':'b', '𝒑':'p', '𝓊':'u', 'н':'h', '𝒸':'c',\n",
    "                 '𝘼':'a','𝘲':'q', 'ｕ':'u', 'ĕ':'e', '𝙛':'f','ν':'v', 'מ':'n', 'ĭ':'i', 'ǐ':'i','å':'a', '𝓽':'t',\n",
    "                 'ἴ':'i', 'ύ':'u', 'ć':'c', 'ä':'a', 'ř':'r', 'ġ':'g', '𝓵':'l','🇻':'v', '𝒕':'t','𝘬':'k','𝗸':'k', 'ａ':'a',\n",
    "                 'ⲏ':'h', 'ᴅ':'d', 'ү':'gamma', '𝖉':'d', 'е':'e','🇫':'f', '𝓻':'r', '𝙪':'u', 'ᴄ':'c','χ':'x','𝑻':'t', \n",
    "                 '𝑥':'x', '𝒇':'f', 'ā':'a', '𝘵':'t','ｄ':'d', 'ɴ':'n','𝑯':'h', 'ｃ':'c','ш':'w','𝒁':'z', '𝐫':'r', \n",
    "                 '𝓲':'i', '𝖈':'c', 'ｐ':'p', 'ᴦ':'r','𝖕':'p', 'ê':'e','𝖂':'b', '𝐯':'v','𝓀':'k',  '𝓮':'e', '𝗞':'k', \n",
    "                 '𝒃':'b', 'ᴍ':'m', 'ρ':'rho', '𝒶':'a', 'ℴ':'o', 'ῖ':'i', '𝗵':'h', '𝙯':'z', 'ô':'o', 'τ':'tau', '𝐩':'p', 'ꭻ':'j',\n",
    "                '\\xad':' ','┈':'....','ʻ':\"'\",'ü':'u','é':'e','ᴇ':'e','è':'e','ғ':'f','𝒊':'i','ʏ':'y','𝒂':'a','ᴋ':'k','𝒏':'n',\n",
    "                 'а':'a','ś':'s', 'í':'i','о':'o','\\x7f':' ','𝒔':'s','𝙚':'e','ö':'o','в':'b','s':'s','▀':' ','▄':' ','➤':' ',\n",
    "                 '═':'=','☻':' smile ','❥':' love ','★':' star ','м':'m','𝙣':'n','𝒅':'d','𝙧':'r','𝒄':'c','ō':'o','𝙨':'s','ᴛ':'t',\n",
    "                 'ē':'e','ᴜ':'u','𝒎':'m','𝙞':'i','υ':'u','ɪ':'i','к':'k','▰':' ','▔':' ','▬':' ','̶':'-','╲':\"\\\\\",'╱':'/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ') # we can try just replace puncts into space\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '','✰':' ','§':' ','○':'','❧':'','ι':''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "temp = temp.apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = temp[:train.shape[0]]\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = temp[train.shape[0]:]\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "pickle_out = open('x_train.pickle','wb')\n",
    "pickle.dump(x_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_aux_train.pickle','wb')\n",
    "pickle.dump(y_aux_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('x_test.pickle','wb')\n",
    "pickle.dump(x_test,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('weights.pickle','wb')\n",
    "pickle.dump(weights,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('loss_weight.pickle','wb')\n",
    "pickle.dump(loss_weight,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_train.pickle','wb')\n",
    "pickle.dump(y_train,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del test\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = preprocess(train['comment_text'])\n",
    "# y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "# x_test = preprocess(test['comment_text'])\n",
    "\n",
    "# identity_columns = [\n",
    "#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# # Overall\n",
    "# weights = np.ones((len(x_train),)) / 4\n",
    "# # Subgroup\n",
    "# weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# # Background Positive, Subgroup Negative\n",
    "# weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# # Background Negative, Subgroup Positive\n",
    "# weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "# y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity_columns = [\n",
    "#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# # Overall\n",
    "# weights = np.ones((len(x_train),)) / 4\n",
    "# # Subgroup\n",
    "# weights += (train_identity_columns.fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# # Background Positive, Subgroup Negative\n",
    "# weights += (( (train_target.values>=0.5).astype(bool).astype(np.int) +\n",
    "#    (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# # Background Negative, Subgroup Positive\n",
    "# weights += (( (train_target.values<0.5).astype(bool).astype(np.int) +\n",
    "#    (train_identity_columns.fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "# y_train = np.vstack([(train_target.values>=0.5).astype(np.int),weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282482"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_no_web = build_vocab(no_web_comment_text)\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 31155 words to embedding\n"
     ]
    }
   ],
   "source": [
    "C_embedding = load_embeddings(CRAWL_EMBEDDING_PATH)\n",
    "add_lower(C_embedding, vocab_no_web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  110238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, C_embedding)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "del C_embedding\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 25381 words to embedding\n",
      "n unknown words (glove):  111000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_embedding = load_embeddings(GLOVE_EMBEDDING_PATH)\n",
    "add_lower(G_embedding, vocab_no_web)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, G_embedding)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "del G_embedding\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vocab_no_web\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_out = open('embedding_matrix.pickle','wb')\n",
    "pickle.dump(embedding_matrix,pickle_out)\n",
    "pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (os.listdir('./'))\n",
    "# pickle_in = open('embedding_matrix.pickle','rb')\n",
    "# test_matrix = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embedding_matrix.shape)\n",
    "# print(test_matrix.shape)\n",
    "# # \n",
    "# np.array_equal(embedding_matrix,test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(x_train)\n",
    "# pickle_out = open('x_train.pickle','wb')\n",
    "# pickle.dump(x_train,pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (os.listdir('./'))\n",
    "# pickle_in = open('x_train.pickle','rb')\n",
    "# test_matrix2 = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array_equal(x_train,test_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_torch = torch.tensor(x_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "\n",
    "# train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "# valid_dataset = data.TensorDataset(x_train_torch[:batch_size], y_train_torch[:batch_size])\n",
    "# test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# databunch = DataBunch(train_dl=train_loader,valid_dl=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(data, targets):\n",
    "#     ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "#     bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "#     bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "#     return (bce_loss_1 * loss_weight) + bce_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_test_preds = []\n",
    "\n",
    "# for model_idx in range(NUM_MODELS):\n",
    "#     print('Model ', model_idx)\n",
    "#     seed_everything(1234 + model_idx)\n",
    "#     model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "#     learn = Learner(databunch,model,loss_func=custom_loss)\n",
    "#     test_preds = train_model(learn,test_dataset,output_dim=7)    \n",
    "#     all_test_preds.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame.from_dict({\n",
    "#     'id': test['id'],\n",
    "#     'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "# })\n",
    "\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public LB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
